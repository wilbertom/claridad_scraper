#!/usr/bin/env python
import logging
import os

from itertools import zip_longest


import click
import dataset

from claridad_scraper import Scraper, DBSink
from claridad import Article, ArticleMetaDataNotFound, Author
from claridad_wordpress import PostExporter, UserExporter, User

def chunk_to_file(chunk, count):
    f = open('articles_'+ str(count) +'.csv', 'w')
    exporter = PostExporter(f)
    for row in chunk:
        # TODO: drop #comments in query
        if row['link'] and row['link'].endswith('#comments'):
            continue

        article = Article(row['id'], row['text'])

        if article.is_summary:
            continue

        try:
            exporter.export(article)
        except ArticleMetaDataNotFound:
            continue

def grouper(n, iterable, padvalue=None):
    "grouper(3, 'abcdefg', 'x') --> ('a','b','c'), ('d','e','f'), ('g','x','x')"
    return zip_longest(*[iter(iterable)]*n, fillvalue=padvalue)

@click.group()
def cli():
    pass


@cli.command()
@click.option('--db-path', help='Path to directory where all data will be saved in.')
@click.option(
    '--site',
    help='Starting point for the scraper, defaults to the home page.',
    default='http://www.claridadpuertorico.com/'
)
def scraper(db_path, site):
    logging.basicConfig(level=logging.INFO)

    db_sink = DBSink(db_path)
    scraper = Scraper(site, db_sink)

    scraper.run()


@cli.command()
@click.option('--db-path', help='Path to directory where all data is saved in.')
@click.option('--chunk', help='Path to directory where all data is saved in.')
def export_posts(db_path, chunk):
    db_sink = DBSink(db_path)
    articles = db_sink.query('content.html?news=')
    chunk_count = 0

    if chunk:
        chunks = grouper(120, articles)
    else:
        chunks = articles

    for chunk in chunks:
        chunk_count += 1
        chunk_to_file(chunk, chunk_count if chunk else 'all')


@cli.command()
@click.option('--db-path', help='Path to directory where all data is saved in.')
@click.option('--to', help='CSV file output.')
@click.option('--starting-id', type=int, help='Starting ID for record, avoids using ids already taken in the database.')
def export_users(db_path, to, starting_id):
    f = open(to, 'w')
    exporter = UserExporter(f)
    db_sink = DBSink(db_path)
    user_id = starting_id

    for row in db_sink.query('perfilautor.html?aid='):
        author = Author(db_sink.text(row))
        user = User(user_id, author.username, author.name, author.email)
        exporter.export(user)
        user_id += 1


if __name__ == '__main__':
    cli()
